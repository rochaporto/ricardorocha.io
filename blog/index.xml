<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blogs on Ricardo Rocha</title><link>https://ricardorocha.io/blog/</link><description>Recent content in Blogs on Ricardo Rocha</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 06 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://ricardorocha.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Building Immutable Cluster Images with LinuxKit</title><link>https://ricardorocha.io/blog/linuxkit-minimal-image/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://ricardorocha.io/blog/linuxkit-minimal-image/</guid><description>TL;DR Immutable infrastructure is a good option to avoid configuration drift LinuxKit can produce minimal images, speeding up deployments and improving security Containerized deployments can benefit from the above One of the challenges of running large scale infrastructures is making sure nodes are kept consistent and up to date, avoiding configuration drift. One way of achieving this is relying on systems like CFEngine, Puppet or Chef, regularly (re)applying the expected configuration on the nodes (SnowFlakes).</description></item><item><title>Generating Prometheus Metrics from Application Logs</title><link>https://ricardorocha.io/blog/prometheus-metrics-mtail/</link><pubDate>Wed, 16 Dec 2020 12:00:00 +0000</pubDate><guid>https://ricardorocha.io/blog/prometheus-metrics-mtail/</guid><description>While many applications and services already expose internal metrics in Prometheus format - especially the ones often deployed on Kubernetes - many others keep this information only inside log files and require extra tooling to parse and expose the data.
Recently i worked on a distributed setup with Kubernetes clusters running in multiple clouds, each with a Prometheus instance collecting local metrics which then get aggregated centrally. One of the use cases involves evaluating the performance of batch like workloads, which also tend to keep performance data in the logs for end user evaluation after the jobs are finished.</description></item><item><title>Generating Prometheus Metrics from Application Logs</title><link>https://ricardorocha.io/blog/ubuntu-setup/</link><pubDate>Wed, 16 Dec 2020 12:00:00 +0000</pubDate><guid>https://ricardorocha.io/blog/ubuntu-setup/</guid><description>While many applications and services already expose internal metrics in Prometheus format - especially the ones often deployed on Kubernetes - many others keep this information only inside log files and require extra tooling to parse and expose the data.
Recently i worked on a distributed setup with Kubernetes clusters running in multiple clouds, each with a Prometheus instance collecting local metrics which then get aggregated centrally. One of the use cases involves evaluating the performance of batch like workloads, which also tend to keep performance data in the logs for end user evaluation after the jobs are finished.</description></item><item><title>A VPN-less Teleworking Setup</title><link>https://ricardorocha.io/blog/teleworking-setup-novpn/</link><pubDate>Mon, 16 Nov 2020 16:00:00 +0000</pubDate><guid>https://ricardorocha.io/blog/teleworking-setup-novpn/</guid><description>Working from home became frequent enough since March this year to justify an improved home work setup (both hardware and software).
Setting up a VPN would be an easy and obvious choice but this is currently not an option at work. Alternatively and after suggestions from multiple people a combination of ssh / proxy setup and a handy browser extension and the result is close enough.
Ignore all the GSSAPI* below if you don&#39;t require Kerberos or similar access.</description></item><item><title>Analysing Prometheus Metrics in Pandas</title><link>https://ricardorocha.io/blog/prometheus-metrics-in-pandas/</link><pubDate>Fri, 13 Nov 2020 22:00:00 +0000</pubDate><guid>https://ricardorocha.io/blog/prometheus-metrics-in-pandas/</guid><description>I spend more and more time working with Prometheus as it&#39;s the default monitoring / metric collection system for Kubernetes clusters at work and other projects i have. It&#39;s also a great tool overall i wish i had for longer. Pair it with Grafana and it gives a well integrated monitoring solution: While a lot can be done with the timeseries data using PromQL and Grafana, i often miss Pandas to easily reorganize the data.</description></item><item><title>Custom Containerd Versions in GKE Clusters</title><link>https://ricardorocha.io/blog/gke-custom-containerd/</link><pubDate>Sun, 25 Oct 2020 22:00:00 +0000</pubDate><guid>https://ricardorocha.io/blog/gke-custom-containerd/</guid><description>GKE (Google Kubernetes Engine) is a strong option for managed Kubernetes cluster deployments, backed by the Google Cloud Platform. Relying on managed services reduces the maintenance overhead and building on features such as auto-repair and auto-upgrade is also a plus.
In some rare cases more flexibility is required, usually involving an early or very experimental feature in a critical component. In this case, we need a more recent version of containerd.</description></item><item><title>Managing Builds and Releases with GitHub Actions</title><link>https://ricardorocha.io/blog/managing-builds-and-releases-with-github-actions/</link><pubDate>Mon, 02 Dec 2019 18:00:00 +0000</pubDate><guid>https://ricardorocha.io/blog/managing-builds-and-releases-with-github-actions/</guid><description>I&#39;ve been a longtime user of Travis CI, specifically for a pet project managing gliding flights parsing and analysis - that in the past had iterations in Java and Python and has for several years been written in Go.
With recent talk about GitHub Actions and also seeing that a couple of new projects in the containers area seem to be using it, i decided to give this a go. The first results are pretty good and in this post i&#39;ll describe what&#39;s available and how to use this functionality to:</description></item></channel></rss>